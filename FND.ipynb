{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install plotly\n",
    "!pip install --upgrade nbformat\n",
    "!pip install nltk\n",
    "!pip install spacy # spaCy is an open-source software library for advanced natural language processing\n",
    "!pip install WordCloud\n",
    "!pip install gensim # Gensim is an open-source library for unsupervised topic modeling and natural language processing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False)\n",
    "# setting the style of the notebook to be monokai theme\n",
    "# this line of code is important to ensure that we are able to see the x and y axes clearly\n",
    "# If you don't run this code line, you will notice that the xlabel and ylabel on any plot is black on black and it will be hard to see them.\n",
    "\n",
    "\n",
    "# load the data\n",
    "df_true = pd.read_csv(\"True.csv\")\n",
    "df_fake = pd.read_csv(\"Fake.csv\")\n",
    "\n",
    "\n",
    "df_fake\n",
    "df_true\n",
    "\n",
    "df_true.isnull().sum()\n",
    "df_fake.isnull().sum()\n",
    "\n",
    "df_true.info()\n",
    "df_fake.info()\n",
    "\n",
    "# add a target class column to indicate whether the news is real or fake\n",
    "df_true['isfake'] = 1\n",
    "df_true.head()\n",
    "\n",
    "df_fake['isfake'] = 0\n",
    "df_fake.head()\n",
    "\n",
    "# Concatenate Real and Fake News\n",
    "df = pd.concat([df_true, df_fake]).reset_index(drop = True)\n",
    "df\n",
    "\n",
    "#reset_index is used to index the concatenated rows from 0 to (sum of df_true+df_fake)\n",
    "#concatenated into one dataframe to see true and fake results in one\n",
    "\n",
    "\n",
    "#drop redundant column\n",
    "\n",
    "df.drop(columns = ['date'], inplace ='TRUE')\n",
    "\n",
    "#inplace = FALSE only drops the column here in notebook (code)\n",
    "#inplace = TRUE will drop the column in memory as well (Permanent change)\n",
    "\n",
    "#combining the columns; title and text to consider it as 1 column in df\n",
    "\n",
    "df['original'] = df['title'] + ' ' + df['text']\n",
    "\n",
    "df['original'][0]  #original column x first row result\n",
    "\n",
    "#Now we need to clean our data i.e. remove stop words from the data\n",
    "\n",
    "\n",
    "#--------TASK 4 -----DATA CLEANING----------\n",
    "\n",
    "nltk.download(\"stopwords\")   #download package stopwords\n",
    "\n",
    "#import stopwords from nltk (NLP)\n",
    "\n",
    "# Obtain additional stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')   #extract english text stopwords\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "#We can extend by adding some words to the stopwords we want to drop from our text\n",
    "\n",
    "\n",
    "stop_words #Results all the stopwords\n",
    "\n",
    "\n",
    "# Remove stopwords and remove words with 2 or less characters\n",
    "def preprocess(text):  #function called preprocess with column text\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):  #gensim is a library used for nlp\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words:\n",
    "            result.append(token)\n",
    "\n",
    "    return result\n",
    "\n",
    "#Starting result as 0, check for a token (variable) if its length is >3 and it is not in the stopwords, then append it to my results\n",
    "\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df['clean'] = df['original'].apply(preprocess)  #Adds new column in df that is without the stopwords\n",
    "\n",
    "df['original'][0]  #Check the original text for comparison\n",
    "\n",
    "print(df['clean'][0])   #Print the clear column x first row\n",
    "\n",
    "#This gives us the unique data that we need to feed to our LSTM RNN by converting them to numbers (tokenization)\n",
    "\n",
    "# Obtain the total words present in the dataset\n",
    "list_of_words = []\n",
    "for i in df.clean:\n",
    "    for j in i:\n",
    "        list_of_words.append(j)\n",
    "\n",
    "\n",
    "list_of_words  #show words in dfclean in entire dataframe\n",
    "\n",
    "len(list_of_words)  #9276947\n",
    "\n",
    "# Obtain the total number of unique words\n",
    "total_words = len(list(set(list_of_words)))   #Applying 'set' gives unique words only\n",
    "total_words        #total unique words = 108704\n",
    "\n",
    "# join the words into a string\n",
    "df['clean_joined'] = df['clean'].apply(lambda x: \" \".join(x))  #Join all words separated by space\n",
    "\n",
    "df\n",
    "\n",
    "df['clean_joined'][0]\n",
    "\n",
    "#-----------TASK-5-------VISUALIZE DATA-------\n",
    "\n",
    "# plot the number of samples in 'subject'\n",
    "plt.figure(figsize = (8, 8))\n",
    "sns.countplot(y = \"subject\", data = df)  #Seaborn library to visualize the column subject\n",
    "\n",
    "#y axis has subject column and data is on the x axis\n",
    "\n",
    "#Print out WordCloud; WordCloud is actually really powerful visualization specifically for text data\n",
    "\n",
    "#shows importance of each word in text (The keywords)\n",
    "\n",
    "# plot the word cloud for text that is Fake\n",
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(df[df.isfake == 0].clean_joined))\n",
    "plt.imshow(wc, interpolation = 'bilinear')\n",
    "\n",
    "#maxwords is the maximum words you want to print out on screen\n",
    "# plot the word cloud for text that is Real\n",
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(df[df.isfake == 1].clean_joined))\n",
    "plt.imshow(wc, interpolation = 'bilinear')\n",
    "\n",
    "# length of maximum document will be needed to create word embeddings\n",
    "maxlen = -1\n",
    "for doc in df.clean_joined:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    if(maxlen<len(tokens)):\n",
    "        maxlen = len(tokens)\n",
    "print(\"The maximum number of words in any document is =\", maxlen)\n",
    "\n",
    "#nltk.word_tokenize(df['clean_joined'][0])  would tokenize each word in a string eg ['hi', 'hello', 'new']\n",
    "\n",
    "# visualize the distribution of number of words in a text\n",
    "import plotly.express as px\n",
    "fig = px.histogram(x = [len(nltk.word_tokenize(x)) for x in df.clean_joined], nbins = 100)\n",
    "fig.show()\n",
    "\n",
    "#-------TASK 6------- PREPARE DATA USING TOKENIZATION AND PADDING\n",
    "\n",
    "\n",
    "#Split data into train and test using sklearn\n",
    "\n",
    "# split data into test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.clean_joined, df.isfake, test_size = 0.2)\n",
    "\n",
    "#Parameters include input data as df.clean joined and output as df.isfake, the test size is 20% here meaning\n",
    "#testing data = 20% and training data = 80%\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Create a tokenizer to tokenize the words and create sequences of tokenized words\n",
    "tokenizer = Tokenizer(num_words = total_words)\n",
    "tokenizer.fit_on_texts(x_train)  #Stack overflow - Vocabulary\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train)  #Transformation here to integers\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "len(train_sequences)\n",
    "\n",
    "print(\"The encoding for document\\n\",df.clean_joined[0],\"\\n is : \",train_sequences[0])\n",
    "\n",
    "#All the different news should have the same length, for that we use padding\n",
    "\n",
    "# Add padding can either be maxlen = 4406 or smaller number maxlen = 40 seems to work well based on results\n",
    "padded_train = pad_sequences(train_sequences,maxlen = 40, padding = 'post', truncating = 'post')\n",
    "padded_test = pad_sequences(test_sequences,maxlen = 40, truncating = 'post')\n",
    "\n",
    "#can take 4406 too\n",
    "#Zeros are added at the end of each sample to make all samples of same length\n",
    "#for loop to check padded sequences in first two rows\n",
    "for i,doc in enumerate(padded_train[:2]):\n",
    "     print(\"The padded encoding for document\",i+1,\" is : \",doc)\n",
    "\n",
    "\n",
    "\n",
    "# Sequential Model\n",
    "model = Sequential()\n",
    "\n",
    "# embeddidng layer\n",
    "model.add(Embedding(total_words, output_dim = 128))   #can change the output dimension\n",
    "# model.add(Embedding(total_words, output_dim = 240))\n",
    "\n",
    "\n",
    "# Bi-Directional RNN and LSTM\n",
    "model.add(Bidirectional(LSTM(128)))  #can change the number of nuerons to add\n",
    "\n",
    "# Dense layers\n",
    "model.add(Dense(128, activation = 'relu'))  #128 layers and relu activation\n",
    "model.add(Dense(1,activation= 'sigmoid'))  #sigmoid layer, 1 neuron (output)  which represents 0 or 1\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])  #metrics = accuracy\n",
    "model.summary()\n",
    "\n",
    "#check .compile etc functions\n",
    "\n",
    "y_train = np.asarray(y_train) # converting to array, imp step before feeding along the model\n",
    "\n",
    "# train the model\n",
    "model.fit(padded_train, y_train, batch_size = 64, validation_split = 0.1, epochs = 2)\n",
    "\n",
    "#input = padded_train, output = (array) y_train\n",
    "#validation spilt = 10% for cross validation and 90 for training the model (i.e. splitting training data)\n",
    "#we apply cross validation to check the model is not over fitting the data. After every epoch, the 10%..\n",
    "#..will run through the model and see if error of validation is going down or not. If down, then good.\n",
    "#If the error on training and validation is going down , then good\n",
    "#If the error on training data is going down but the error on validation is going up, then over fitting, hence fix\n",
    "\n",
    "#Result for two epoch after running\n",
    "#Train on 32326 samples, validate on 3592 samples\n",
    "#Epoch 1/2\n",
    "#32326/32326 [==============================] - 321s 10ms/sample - loss: 0.0421 - acc: 0.9815 - val_loss: 0.0073 - val_acc: 0.9992\n",
    "#Epoch 2/2\n",
    "#32326/32326 [==============================] - 316s 10ms/sample - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0096 - val_acc: 0.9981\n",
    "\n",
    "#Here above, the result shows accuracy of 99%\n",
    "\n",
    "\n",
    "#----------TASK 9---------ASSESS TRAINED MODEL  (On testing data)\n",
    "\n",
    "# make prediction\n",
    "pred = model.predict(padded_test)  #feed testing data\n",
    "\n",
    "# if the predicted value is >0.5 it is real else it is fake\n",
    "#Applying threshold for our output (which in our case is a sigmoid function i.e (i.e. either 0 or 1))\n",
    "\n",
    "prediction = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i].item() > 0.5:   #So, here threshold = 0.5\n",
    "        prediction.append(1)  #put into class real\n",
    "    else:\n",
    "        prediction.append(0)\n",
    "\n",
    "# getting the accuracy\n",
    "#model predictions vs the actual\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(list(y_test), prediction)\n",
    "\n",
    "print(\"Model Accuracy : \", accuracy)   # we get 0.99\n",
    "\n",
    "# get the confusion matrix\n",
    "#i.e. visualising the predictions vs actual\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(list(y_test), prediction)\n",
    "plt.figure(figsize = (25, 25))\n",
    "sns.heatmap(cm, annot = True)\n",
    "\n",
    "#this tells us the misclassified samples visually\n",
    "\n",
    "# category dict\n",
    "category = { 0: 'Fake News', 1 : \"Real News\"}  #final labels\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
